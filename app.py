import streamlit as st
import pandas as pd
import praw
from mastodon import Mastodon
import requests
import random
import os
from transformers import pipeline
from datetime import datetime
import re

# ---------------------- INITIALISATION ---------------------- #

# Chargement des clÃ©s API pour Reddit et Mastodon
def load_reddit_api():
    reddit = praw.Reddit(
        client_id=st.secrets.get("REDDIT_CLIENT_ID"),
        client_secret=st.secrets.get("REDDIT_CLIENT_SECRET"),
        user_agent=st.secrets.get("REDDIT_USER_AGENT")
    )
    return reddit


def load_mastodon_api():
    mastodon = Mastodon(
        access_token=st.secrets.get("MASTODON_ACCESS_TOKEN"),
        api_base_url=st.secrets.get("MASTODON_API_BASE_URL")
    )
    return mastodon

# Chargement du pipeline d'analyse de sentiment avec un modÃ¨le lÃ©ger
sentiment_analyzer = pipeline("sentiment-analysis", model="distilbert-base-uncased-finetuned-sst-2-english", device=-1)

if "combined_data" not in st.session_state:
    st.session_state.combined_data = pd.DataFrame(columns=["Plateforme", "Date", "Utilisateur", "Texte", "Sentiment", "Score"])

# ---------------------- FONCTIONS UTILES ---------------------- #

def clean_text(text):
    """Nettoie le texte en supprimant le HTML et les caractÃ¨res spÃ©ciaux."""
    text = re.sub(r'<.*?>', '', text)  # Supprimer les balises HTML
    text = re.sub(r'[^A-Za-zÃ€-Ã–Ã˜-Ã¶Ã¸-Ã¿0-9 .,!?]', '', text)  # CaractÃ¨res autorisÃ©s
    return text.strip()

def analyze_sentiment(text):
    if not text or text.isspace():
        return "Neutral", 0.0

    cleaned_text = clean_text(text)[:512]  # Limite de 512 caractÃ¨res
    try:
        result = sentiment_analyzer(cleaned_text)[0]
        return result['label'], round(result['score'] * 100, 2)
    except Exception as e:
        st.warning(f"Erreur d'analyse du texte : {e}")
        return "Error", 0.0

def save_combined_data():
    st.session_state.combined_data.to_csv("combined_data.csv", index=False)

def load_combined_data():
    if os.path.exists("combined_data.csv"):
        st.session_state.combined_data = pd.read_csv("combined_data.csv")

load_combined_data()

# ---------------------- COLLECTE DE DONNÃ‰ES ---------------------- #

def collect_reddit_posts(reddit, subreddit_name, limit=10):
    posts = []
    try:
        subreddit = reddit.subreddit(subreddit_name)
        for post in subreddit.hot(limit=limit):
            sentiment, score = analyze_sentiment(post.title + " " + (post.selftext or ""))
            posts.append({
                "Plateforme": "Reddit",
                "Date": datetime.fromtimestamp(post.created_utc).strftime('%Y-%m-%d %H:%M:%S'),
                "Utilisateur": post.author.name if post.author else "Anonyme",
                "Texte": post.title + " " + (post.selftext or ""),
                "Sentiment": sentiment,
                "Score": score
            })
    except Exception as e:
        st.error(f"ğŸš« Erreur lors de la collecte Reddit : {e}")
    return posts

def collect_mastodon_toots(mastodon, hashtag, limit=10):
    posts = []
    try:
        toots = mastodon.timeline_hashtag(hashtag, limit=limit)
        for toot in toots:
            content = toot['content']
            sentiment, score = analyze_sentiment(content)
            posts.append({
                "Plateforme": "Mastodon",
                "Date": toot['created_at'],
                "Utilisateur": toot['account']['username'],
                "Texte": content,
                "Sentiment": sentiment,
                "Score": score
            })
    except Exception as e:
        st.error(f"ğŸš« Erreur lors de la collecte Mastodon : {e}")
    return posts

def load_open_source_dataset(file):
    try:
        df = pd.read_csv(file)
        df['Texte'] = df['Texte'].astype(str)
        df['Sentiment'], df['Score'] = zip(*df['Texte'].map(analyze_sentiment))
        df['Plateforme'] = 'Dataset'
        return df[['Plateforme', 'Date', 'Utilisateur', 'Texte', 'Sentiment', 'Score']]
    except Exception as e:
        st.error(f"ğŸš« Erreur lors de l'import du dataset : {e}")
        return pd.DataFrame()

# ---------------------- GÃ‰NÃ‰RATION DE CONTENU ---------------------- #

def generate_trend_based_post(data):
    if data.empty:
        return "Rien de nouveau pour le moment. Restez connectÃ©s !"

    frequent_words = pd.Series(' '.join(data['Texte']).lower().split()).value_counts().head(5).index.tolist()
    trend = random.choice(frequent_words) if frequent_words else "innovation"
    top_sentiment = data['Sentiment'].mode()[0]

    post_templates = [
        f"La discussion sur #{trend} est en plein essor aujourd'hui. Partagez vos pensÃ©es !",
        f"Les utilisateurs ressentent principalement un sentiment {top_sentiment.lower()} autour de #{trend}. Qu'en pensez-vous ?",
        f"#{trend} est au cÅ“ur des dÃ©bats. Voici ce qui est dit : {random.choice(data['Texte'].tolist())}"
    ]

    return random.choice(post_templates)

# ---------------------- INTERFACE STREAMLIT ---------------------- #

st.title("ğŸŒ Agent AI Multiplateforme - Reddit | Mastodon | Open Datasets")

col1, col2, col3 = st.columns(3)

# Collecte sur Reddit
with col1:
    st.subheader("ğŸ“¥ Collecte Reddit")
    subreddit_name = st.text_input("Nom du subreddit:", "cryptocurrency")
    reddit_limit = st.slider("Nombre de posts Ã  collecter:", 1, 20, 10)
    if st.button("Collecter depuis Reddit"):
        reddit = load_reddit_api()
        reddit_posts = collect_reddit_posts(reddit, subreddit_name, reddit_limit)
        if reddit_posts:
            st.session_state.combined_data = pd.concat([st.session_state.combined_data, pd.DataFrame(reddit_posts)], ignore_index=True)
            save_combined_data()
            st.success(f"âœ… {len(reddit_posts)} posts collectÃ©s depuis Reddit.")

# Collecte sur Mastodon
with col2:
    st.subheader("ğŸ˜ Collecte Mastodon")
    hashtag = st.text_input("Hashtag Ã  suivre (sans #):", "blockchain")
    mastodon_limit = st.slider("Nombre de toots Ã  collecter:", 1, 20, 10)
    if st.button("Collecter depuis Mastodon"):
        mastodon = load_mastodon_api()
        mastodon_posts = collect_mastodon_toots(mastodon, hashtag, mastodon_limit)
        if mastodon_posts:
            st.session_state.combined_data = pd.concat([st.session_state.combined_data, pd.DataFrame(mastodon_posts)], ignore_index=True)
            save_combined_data()
            st.success(f"âœ… {len(mastodon_posts)} toots collectÃ©s depuis Mastodon.")

# Import de jeux de donnÃ©es
with col3:
    st.subheader("ğŸ“‚ Importer un dataset")
    uploaded_file = st.file_uploader("Choisir un fichier CSV:")
    if uploaded_file:
        dataset_df = load_open_source_dataset(uploaded_file)
        if not dataset_df.empty:
            st.session_state.combined_data = pd.concat([st.session_state.combined_data, dataset_df], ignore_index=True)
            save_combined_data()
            st.success("âœ… DonnÃ©es du fichier importÃ©es avec succÃ¨s.")

st.markdown("---")

st.subheader("ğŸ“Š Visualisation des donnÃ©es combinÃ©es")
if not st.session_state.combined_data.empty:
    st.dataframe(st.session_state.combined_data.head(20))
    sentiment_counts = st.session_state.combined_data['Sentiment'].value_counts()
    st.bar_chart(sentiment_counts)
else:
    st.info("Aucune donnÃ©e collectÃ©e pour le moment.")

st.markdown("---")

st.subheader("âœï¸ GÃ©nÃ©ration de contenu basÃ© sur les tendances")
if st.button("GÃ©nÃ©rer un post basÃ© sur les tendances"):
    generated_post = generate_trend_based_post(st.session_state.combined_data)
    st.success(f"ğŸ“ Post gÃ©nÃ©rÃ© : {generated_post}")

st.markdown("---")

st.subheader("ğŸ’¾ Exporter les donnÃ©es")
if not st.session_state.combined_data.empty:
    csv = st.session_state.combined_data.to_csv(index=False).encode('utf-8')
    st.download_button(
        label="ğŸ“¥ TÃ©lÃ©charger les donnÃ©es combinÃ©es en CSV",
        data=csv,
        file_name="combined_data.csv",
        mime="text/csv",
    )
else:
    st.info("Collectez ou importez des donnÃ©es avant de pouvoir les tÃ©lÃ©charger.")
