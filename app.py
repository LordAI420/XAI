import streamlit as st
import pandas as pd
import praw
from mastodon import Mastodon
import requests
import random
import os
from transformers import pipeline
from datetime import datetime

# ---------------------- INITIALISATION ---------------------- #

# Chargement des clÃ©s API pour Reddit et Mastodon
def load_reddit_api():
    reddit = praw.Reddit(
        client_id=st.secrets.get("REDDIT_CLIENT_ID"),
        client_secret=st.secrets.get("REDDIT_CLIENT_SECRET"),
        user_agent=st.secrets.get("REDDIT_USER_AGENT")
    )
    return reddit


def load_mastodon_api():
    mastodon = Mastodon(
        access_token=st.secrets.get("MASTODON_ACCESS_TOKEN"),
        api_base_url=st.secrets.get("MASTODON_API_BASE_URL")
    )
    return mastodon

sentiment_analyzer = pipeline("sentiment-analysis")

if "combined_data" not in st.session_state:
    st.session_state.combined_data = pd.DataFrame(columns=["Plateforme", "Date", "Utilisateur", "Texte", "Sentiment", "Score"])

# ---------------------- COLLECTE DE DONNÃ‰ES ---------------------- #

# Reddit - Collecte de posts
def collect_reddit_posts(reddit, subreddit_name, limit=10):
    subreddit = reddit.subreddit(subreddit_name)
    posts = []
    for post in subreddit.hot(limit=limit):
        sentiment, score = analyze_sentiment(post.title + " " + (post.selftext or ""))
        posts.append({
            "Plateforme": "Reddit",
            "Date": datetime.fromtimestamp(post.created_utc).strftime('%Y-%m-%d %H:%M:%S'),
            "Utilisateur": post.author.name if post.author else "Anonyme",
            "Texte": post.title + " " + (post.selftext or ""),
            "Sentiment": sentiment,
            "Score": score
        })
    return posts

# Mastodon - Collecte de toots
def collect_mastodon_toots(mastodon, hashtag, limit=10):
    toots = mastodon.timeline_hashtag(hashtag, limit=limit)
    posts = []
    for toot in toots:
        content = toot['content']
        sentiment, score = analyze_sentiment(content)
        posts.append({
            "Plateforme": "Mastodon",
            "Date": toot['created_at'],
            "Utilisateur": toot['account']['username'],
            "Texte": content,
            "Sentiment": sentiment,
            "Score": score
        })
    return posts

# Jeux de donnÃ©es open source - Import de CSV
def load_open_source_dataset(file):
    df = pd.read_csv(file)
    df['Sentiment'], df['Score'] = zip(*df['Texte'].map(analyze_sentiment))
    df['Plateforme'] = 'Dataset'
    return df[['Plateforme', 'Date', 'Utilisateur', 'Texte', 'Sentiment', 'Score']]

# Analyse de sentiments
def analyze_sentiment(text):
    result = sentiment_analyzer(text)[0]
    return result['label'], round(result['score'] * 100, 2)

# Sauvegarde des donnÃ©es
def save_combined_data():
    st.session_state.combined_data.to_csv("combined_data.csv", index=False)

# Chargement des donnÃ©es existantes
def load_combined_data():
    if os.path.exists("combined_data.csv"):
        st.session_state.combined_data = pd.read_csv("combined_data.csv")

load_combined_data()

# ---------------------- GÃ‰NÃ‰RATION DE CONTENU ---------------------- #

def generate_trend_based_post(data):
    if data.empty:
        return "Rien de nouveau pour le moment. Restez connectÃ©s !"

    frequent_words = pd.Series(' '.join(data['Texte']).lower().split()).value_counts().head(5).index.tolist()
    trend = random.choice(frequent_words) if frequent_words else "innovation"
    top_sentiment = data['Sentiment'].mode()[0]

    post_templates = [
        f"La discussion sur #{trend} est en plein essor aujourd'hui. Partagez vos pensÃ©es !",
        f"Les utilisateurs ressentent principalement un sentiment {top_sentiment.lower()} autour de #{trend}. Qu'en pensez-vous ?",
        f"#{trend} est au cÅ“ur des dÃ©bats. Voici ce qui est dit : {random.choice(data['Texte'].tolist())}"
    ]

    return random.choice(post_templates)

# ---------------------- INTERFACE STREAMLIT ---------------------- #

st.title("ğŸŒ Agent AI Multiplateforme - Reddit | Mastodon | Open Datasets")

col1, col2, col3 = st.columns(3)

# Collecte sur Reddit
with col1:
    st.subheader("ğŸ“¥ Collecte Reddit")
    subreddit_name = st.text_input("Nom du subreddit:", "cryptocurrency")
    reddit_limit = st.slider("Nombre de posts Ã  collecter:", 1, 20, 10)
    if st.button("Collecter depuis Reddit"):
        reddit = load_reddit_api()
        reddit_posts = collect_reddit_posts(reddit, subreddit_name, reddit_limit)
        st.session_state.combined_data = pd.concat([st.session_state.combined_data, pd.DataFrame(reddit_posts)], ignore_index=True)
        save_combined_data()
        st.success(f"âœ… {len(reddit_posts)} posts collectÃ©s depuis Reddit.")

# Collecte sur Mastodon
with col2:
    st.subheader("ğŸ˜ Collecte Mastodon")
    hashtag = st.text_input("Hashtag Ã  suivre (sans #):", "blockchain")
    mastodon_limit = st.slider("Nombre de toots Ã  collecter:", 1, 20, 10)
    if st.button("Collecter depuis Mastodon"):
        mastodon = load_mastodon_api()
        mastodon_posts = collect_mastodon_toots(mastodon, hashtag, mastodon_limit)
        st.session_state.combined_data = pd.concat([st.session_state.combined_data, pd.DataFrame(mastodon_posts)], ignore_index=True)
        save_combined_data()
        st.success(f"âœ… {len(mastodon_posts)} toots collectÃ©s depuis Mastodon.")

# Import de jeux de donnÃ©es
with col3:
    st.subheader("ğŸ“‚ Importer un dataset")
    uploaded_file = st.file_uploader("Choisir un fichier CSV:")
    if uploaded_file:
        dataset_df = load_open_source_dataset(uploaded_file)
        st.session_state.combined_data = pd.concat([st.session_state.combined_data, dataset_df], ignore_index=True)
        save_combined_data()
        st.success("âœ… DonnÃ©es du fichier importÃ©es avec succÃ¨s.")

st.markdown("---")

st.subheader("ğŸ“Š Visualisation des donnÃ©es combinÃ©es")
if not st.session_state.combined_data.empty:
    st.dataframe(st.session_state.combined_data.head(20))
    sentiment_counts = st.session_state.combined_data['Sentiment'].value_counts()
    st.bar_chart(sentiment_counts)
else:
    st.info("Aucune donnÃ©e collectÃ©e pour le moment.")

st.markdown("---")

st.subheader("âœï¸ GÃ©nÃ©ration de contenu basÃ© sur les tendances")
if st.button("GÃ©nÃ©rer un post basÃ© sur les tendances"):
    generated_post = generate_trend_based_post(st.session_state.combined_data)
    st.success(f"ğŸ“ Post gÃ©nÃ©rÃ© : {generated_post}")

st.markdown("---")

st.subheader("ğŸ’¾ Exporter les donnÃ©es")
if not st.session_state.combined_data.empty:
    csv = st.session_state.combined_data.to_csv(index=False).encode('utf-8')
    st.download_button(
        label="ğŸ“¥ TÃ©lÃ©charger les donnÃ©es combinÃ©es en CSV",
        data=csv,
        file_name="combined_data.csv",
        mime="text/csv",
    )
else:
    st.info("Collectez ou importez des donnÃ©es avant de pouvoir les tÃ©lÃ©charger.")
